"""
===============================================================================
MOVIEPY EXPORT IMPLEMENTATION FOR CAPCUT API (Single-file, modular)
===============================================================================

This refactor replaces explicit FFmpeg filtergraphs with MoviePy compositing.
It preserves your concepts:

- Script ‚Üí Tracks ‚Üí Segments ‚Üí Engine ‚Üí Export
- Modular helpers for: sizing, transforms, timing, transitions, audio mixing
- One-file layout, but highly sectioned & testable

Notes:
- MoviePy relies on ffmpeg under the hood, but you no longer construct commands.
- Text rendering uses MoviePy's TextClip (requires ImageMagick or FreeType/FFmpeg).
- Fonts: optional pyfonts mapping preserved; if unavailable, falls back to family name.

Current feature parity (approx):
‚úì add_video/add_image/add_text/add_sticker/add_audio
‚úì z-index compositing, normalized ‚Üí pixel positioning
‚úì scale/rotation/opacity/speed
‚úì multiple tracks mixed; complex timing
‚úì simple "Pull In/Out" transitions via crossfade + zoom
‚úì YAML or draft_id input path; same CLI

Gaps vs your FFmpeg version:
- Effect coverage still basic (blur/sharpen omitted here; can be added with vfx)
- Advanced keyframes/effects not ported
- Text backgrounds/rounded boxes are simplified (can be drawn via PIL mask if needed)

===============================================================================
"""

import os
import sys
import json
import math
import tempfile
import argparse
import logging
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

# --- External deps you already use ---
import pyJianYingDraft as draft  # your CapCut parser
from save_draft_impl import query_script_impl
from pyJianYingDraft.metadata.capcut_transition_meta import TRANSITION_NAME_LUT

# Optional font helper (as in your original code)
try:
    import pyfonts  # type: ignore
    _HAS_PYFONTS = True
except Exception:
    pyfonts = None
    _HAS_PYFONTS = False

# --- MoviePy ---
from moviepy import (
    VideoFileClip,
    AudioFileClip,
    ImageClip,
    TextClip,
    ColorClip,
    CompositeVideoClip,
    CompositeAudioClip,
)

# ---------------------------------------
# Logging
# ---------------------------------------
logger = logging.getLogger(__name__)

# ---------------------------------------
# Font mapping & text sizing calibration
# ---------------------------------------
CAPCUT_TO_GOOGLE_FONT = {
    'Roboto_BlkCn': 'Roboto Condensed',
    'Poppins_Regular': 'Poppins',
    'Poppins_Bold': 'Poppins',
    'Nunito': 'Nunito',
    'PlayfairDisplay_Bold': 'Playfair Display',
    'Pacifico_Regular': 'Pacifico',
    'Caveat_Regular': 'Caveat',
    'Grandstander_Regular': 'Grandstander',
    'DMSans_BoldItalic': 'DM Sans',
    'Exo': 'Exo',
    'Cabin_Rg': 'Cabin',
    'Kanit_Regular': 'Kanit',
    'Kanit_Black': 'Kanit',
    'Staatliches_Regular': 'Staatliches',
    'Bungee_Regular': 'Bungee',
    'Inter_Black': 'Inter',
    'Sora_Regular': 'Sora',
    'Zapfino': 'Great Vibes',
    'Coiny_Regular': 'Coiny',
    'Giveny': 'Bodoni Moda',
}

CC_TEXT_PX_AT_SIZE12: float = 60.0
CC_TEXT_PX_AT_SIZE8: float = 40.0
CC_TEXT_BASE_HEIGHT: float = 1920.0


# ---------------------------------------
# Data classes
# ---------------------------------------
@dataclass
class VideoExportConfig:
    output_path: str
    width: Optional[int] = None       # default to script width
    height: Optional[int] = None      # default to script height
    fps: int = 30
    video_bitrate: str = "8000k"
    audio_bitrate: str = "128k"
    audio_codec: str = "aac"
    audio_channels: int = 2
    audio_sample_rate: int = 44100
    codec: str = "libx264"
    preset: str = "medium"
    crf: str = "23"


@dataclass
class CompositionSegment:
    track_name: str
    track_type: str
    render_index: int
    start_time: float
    end_time: float
    segment_data: Any
    material_data: Any = None
    z_index: int = 0


# ---------------------------------------
# Font helpers
# ---------------------------------------
def _infer_font_style_tokens(capcut_font_name: Optional[str], style_obj: Optional[object]) -> Tuple[str, bool]:
    italic = False
    weight_token = 'regular'

    if style_obj is not None:
        try:
            italic = bool(getattr(style_obj, 'italic', False) or getattr(style_obj, 'is_italic', False))
        except Exception:
            pass
        try:
            w = getattr(style_obj, 'weight', None)
            if isinstance(w, (int, float)):
                if w >= 900: weight_token = 'black'
                elif w >= 800: weight_token = 'extra-bold'
                elif w >= 700: weight_token = 'bold'
                elif w >= 600: weight_token = 'semi-bold'
                elif w >= 500: weight_token = 'medium'
                elif w >= 300: weight_token = 'light'
                elif w >= 200: weight_token = 'extra-light'
                else: weight_token = 'regular'
        except Exception:
            pass

    name = (capcut_font_name or '').lower()
    if 'italic' in name: italic = True
    if any(tok in name for tok in ['black', 'blk']): weight_token = 'black'
    elif 'extrabold' in name or 'extra_bold' in name or 'xtrabold' in name: weight_token = 'extra-bold'
    elif 'semibold' in name or 'semi_bold' in name: weight_token = 'semi-bold'
    elif 'bold' in name: weight_token = 'bold'
    elif 'medium' in name: weight_token = 'medium'
    elif 'extralight' in name or 'extra_light' in name: weight_token = 'extra-light'
    elif 'light' in name: weight_token = 'light'
    elif 'thin' in name: weight_token = 'thin'

    return weight_token, italic


def _resolve_font_name(style_obj: Optional[object], segment_obj: Optional[object]) -> Optional[str]:
    capcut_font_name: Optional[str] = None
    if style_obj is not None:
        for attr in ('font', 'font_name', 'family', 'fontFamily', 'font_family'):
            try:
                v = getattr(style_obj, attr, None)
                if isinstance(v, str) and v.strip():
                    capcut_font_name = v.strip()
                    break
            except Exception:
                pass

    if not capcut_font_name and segment_obj is not None:
        try:
            seg_font = getattr(segment_obj, 'font', None)
            seg_font_name = getattr(seg_font, 'name', None) if seg_font is not None else None
            if isinstance(seg_font_name, str) and seg_font_name.strip():
                capcut_font_name = seg_font_name.strip()
        except Exception:
            pass
        if not capcut_font_name:
            try:
                ranges = getattr(segment_obj, 'text_styles', []) or []
                for r in ranges:
                    rf = getattr(r, 'font', None)
                    rf_name = getattr(rf, 'name', None) if rf is not None else None
                    if isinstance(rf_name, str) and rf_name.strip():
                        capcut_font_name = rf_name.strip()
                        break
            except Exception:
                pass

    if not capcut_font_name:
        return None

    gf_family = CAPCUT_TO_GOOGLE_FONT.get(capcut_font_name) or \
                CAPCUT_TO_GOOGLE_FONT.get(capcut_font_name.replace('-', '_')) or \
                CAPCUT_TO_GOOGLE_FONT.get(capcut_font_name.split('-', 1)[0], capcut_font_name)
    return gf_family


# ---------------------------------------
# Engine
# ---------------------------------------
class MoviePyCompositionEngine:
    def __init__(self, script: 'draft.Script_file', export_cfg: VideoExportConfig):
        self.script = script
        self.width = export_cfg.width or script.width
        self.height = export_cfg.height or script.height
        self.fps = export_cfg.fps or getattr(script, 'fps', 30) or 30
        self.duration_seconds = script.duration / 1_000_000.0
        self.export_cfg = export_cfg

        self.segments: List[CompositionSegment] = []
        self._extract_segments()
        # global z-order: render_index then z_index
        self.segments.sort(key=lambda s: (s.render_index, s.z_index, s.start_time))

        # For transitions between adjacent visual clips on a track
        self.last_visual_by_track: Dict[str, CompositionSegment] = {}

    # ---------- extraction ----------
    def _extract_segments(self):
        track_list = list(self.script.tracks.values())
        track_list.extend(getattr(self.script, 'imported_tracks', []))

        for track in track_list:
            track_name = getattr(track, 'name', 'unnamed')
            track_type = track.track_type.name
            render_index = track.render_index
            for i, segment in enumerate(getattr(track, 'segments', [])):
                tr = getattr(segment, 'target_timerange', None)
                if tr is None:
                    continue
                start_time = tr.start / 1_000_000.0
                end_time = (tr.start + tr.duration) / 1_000_000.0

                material_data = None
                if hasattr(segment, 'material_instance'):
                    material_data = segment.material_instance
                elif hasattr(segment, 'material_id'):
                    material_data = self._find_material_by_id(segment.material_id, track_type)

                self.segments.append(
                    CompositionSegment(
                        track_name=track_name,
                        track_type=track_type,
                        render_index=render_index,
                        start_time=start_time,
                        end_time=end_time,
                        segment_data=segment,
                        material_data=material_data,
                        z_index=i,
                    )
                )

    def _find_material_by_id(self, material_id: str, track_type: str) -> Optional[Any]:
        materials = getattr(self.script, 'materials', None)
        if materials is None:
            return None
        if track_type == 'video':
            for v in getattr(materials, 'videos', []):
                if getattr(v, 'material_id', None) == material_id:
                    return v
        elif track_type == 'audio':
            for a in getattr(materials, 'audios', []):
                if getattr(a, 'material_id', None) == material_id:
                    return a
        elif track_type == 'text':
            for t in getattr(materials, 'texts', []):
                if t.get('id') == material_id:
                    return t
        return None

    # ---------- helpers ----------
    def _map_capcut_size_to_pixels(self, capcut_size: Optional[float]) -> int:
        if capcut_size is None:
            return max(12, int(0.05 * self.height))
        p12, p8, base_h = CC_TEXT_PX_AT_SIZE12, CC_TEXT_PX_AT_SIZE8, CC_TEXT_BASE_HEIGHT or 1920.0
        try:
            slope = (p12 - p8) / (12.0 - 8.0)
            intercept = p12 - slope * 12.0
            px_at_base = slope * float(capcut_size) + intercept
            return max(12, int(px_at_base * (self.height / base_h)))
        except Exception:
            return max(12, int(float(capcut_size) * (self.height / 384.0)))

    def _norm_to_px(self, nx: float, ny: float) -> Tuple[int, int]:
        # CapCut normalized space in [-1, 1] from center
        x = int((nx + 1.0) * self.width / 2)
        y = int((ny + 1.0) * self.height / 2)
        return x, y

    def _pos_expr(self, clip_w: int, clip_h: int, nx: float, ny: float) -> Tuple[int, int]:
        cx, cy = self._norm_to_px(nx, ny)
        return int(cx - clip_w / 2), int(cy - clip_h / 2)

    def _speed_factor(self, segment_data: Any) -> float:
        sp = getattr(segment_data, 'speed', None)
        try:
            f = float(getattr(sp, 'speed', 1.0)) if sp is not None else 1.0
        except Exception:
            f = 1.0
        return max(1e-6, f)

    def _get_source_subclip_bounds(self, segment_data: Any, fallback_duration: float) -> Tuple[float, float]:
        sr = getattr(segment_data, 'source_timerange', None)
        if sr:
            start = sr.start / 1_000_000.0
            dur = sr.duration / 1_000_000.0
            return start, start + dur
        return 0.0, fallback_duration

    # ---------- clip builders ----------
    def _build_video_clip(self, seg: CompositionSegment) -> Optional[VideoFileClip]:
        m = seg.material_data
        url = getattr(m, 'remote_url', None) if m else None
        if not url:
            return None

        seg_duration = max(0.0, seg.end_time - seg.start_time)
        is_image = str(url).lower().split('?')[0].endswith(('.png', '.jpg', '.jpeg', '.webp', '.bmp', '.tiff', '.tif'))

        if is_image:
            base = ImageClip(url, duration=seg_duration)
        else:
            base = VideoFileClip(url)
            s0, s1 = self._get_source_subclip_bounds(seg.segment_data, base.duration)
            base = base.subclip(s0, min(s1, base.duration))

        # transforms
        cs = getattr(seg.segment_data, 'clip_settings', None)
        # scale
        sx, sy = 1.0, 1.0
        if cs is not None:
            sx = float(getattr(cs, 'scale_x', 1.0) or 1.0)
            sy = float(getattr(cs, 'scale_y', 1.0) or 1.0)
        if (sx != 1.0) or (sy != 1.0):
            if abs(sx - sy) < 1e-6:
                base = base.resized(sx)
            else:
                # approximate anisotropic scale: resize to (w*sx, h*sy)
                w, h = base.size
                base = base.resized(newsize=(int(w * sx), int(h * sy)))

        # rotation (degrees, center by default)
        rot = float(getattr(cs, 'rotation', 0.0) or 0.0) if cs is not None else 0.0
        if abs(rot) > 1e-6:
            base = base.rotated(rot)

        # opacity
        alpha = float(getattr(cs, 'alpha', 1.0) or 1.0) if cs is not None else 1.0
        if alpha < 1.0:
            base = base.with_opacity(alpha)

        # speed
        sp = self._speed_factor(seg.segment_data)
        if abs(sp - 1.0) > 1e-6:
            # MoviePy v2: prefer immutable API; use speed effect via with_speed if available
            try:
                base = base.with_speed(sp)
            except Exception:
                # Fallback for environments still exposing v1 API
                from moviepy import vfx as _vfx  # type: ignore
                base = base.fx(_vfx.speedx, sp)

        # position
        nx = float(getattr(cs, 'transform_x', 0.0) or 0.0) if cs is not None else 0.0
        ny = float(getattr(cs, 'transform_y', 0.0) or 0.0) if cs is not None else 0.0
        x_px, y_px = self._pos_expr(*base.size, nx, ny)

        # timing
        base = base.with_start(seg.start_time).with_end(seg.end_time).with_position((x_px, y_px))

        return base

    def _build_sticker_clip(self, seg: CompositionSegment) -> Optional[ColorClip]:
        # placeholder rectangle sticker
        seg_duration = max(0.0, seg.end_time - seg.start_time)
        size = (100, 100)
        color = (255, 0, 0)
        clip = ColorClip(size=size, color=color, duration=seg_duration)

        cs = getattr(seg.segment_data, 'clip_settings', None)
        alpha = float(getattr(cs, 'alpha', 1.0) or 1.0) if cs is not None else 1.0
        if alpha < 1.0:
            clip = clip.with_opacity(alpha)

        nx = float(getattr(cs, 'transform_x', 0.0) or 0.0) if cs is not None else 0.0
        ny = float(getattr(cs, 'transform_y', 0.0) or 0.0) if cs is not None else 0.0
        x_px, y_px = self._pos_expr(*clip.size, nx, ny)

        return clip.with_start(seg.start_time).with_end(seg.end_time).with_position((x_px, y_px))

    def _build_text_clip(self, seg: CompositionSegment) -> Optional[TextClip]:
        sd = seg.segment_data
        txt = getattr(sd, 'text', '') or ''
        if not txt:
            return None

        style = getattr(sd, 'style', None)
        fontsize = self._map_capcut_size_to_pixels(getattr(style, 'size', None)) if style else max(12, int(0.05 * self.height))

        # color
        fontcolor = "white"
        if style and hasattr(style, 'color'):
            try:
                r, g, b = getattr(style, 'color', (1.0, 1.0, 1.0))
                ri, gi, bi = int(round(r*255)), int(round(g*255)), int(round(b*255))
                fontcolor = f"rgb({ri},{gi},{bi})"
            except Exception:
                pass

        font_family = _resolve_font_name(style, sd) or "Arial"

        seg_duration = max(0.0, seg.end_time - seg.start_time)
        txt_clip = TextClip(
            txt,
            fontsize=fontsize,
            color=fontcolor,
            font=font_family,   # v2 requires explicit path; we resolve family via pyfonts elsewhere when available
            method="caption",
            size=(self.width, None)
        )

        # background box (simple): draw a semi-transparent rect by Composite
        bg_data = getattr(sd, 'background', None)
        if bg_data:
            try:
                alpha = float(getattr(bg_data, 'alpha', 0.0) or 0.0)
            except Exception:
                alpha = 0.0
        else:
            alpha = 0.0

        cs = getattr(sd, 'clip_settings', None)
        nx = float(getattr(cs, 'transform_x', 0.0) or 0.0) if cs is not None else 0.0
        ny = float(getattr(cs, 'transform_y', 0.0) or 0.0) if cs is not None else 0.0

        # Position center of text at normalized coord
        # We need final text size; make a temp to obtain its size:
        tmp = txt_clip.with_duration(0.1)
        tw, th = tmp.size
        x_px, y_px = self._pos_expr(tw, th, nx, ny)

        txt_clip = txt_clip.with_start(seg.start_time).with_end(seg.end_time).with_position((x_px, y_px))

        if alpha > 0:
            # crude box slightly larger than text
            pad = int(0.1 * th)
            box = ColorClip(size=(tw + 2*pad, th + 2*pad), color=(0, 0, 0)).with_opacity(alpha)
            box = box.with_start(seg.start_time).with_end(seg.end_time).with_position((x_px - pad, y_px - pad))
            return CompositeVideoClip([box, txt_clip], size=(self.width, self.height)).with_start(seg.start_time).with_end(seg.end_time)

        return txt_clip

    def _build_audio_clip(self, seg: CompositionSegment) -> Optional[AudioFileClip]:
        m = seg.material_data
        url = getattr(m, 'remote_url', None) if m else None
        if not url:
            return None
        base = AudioFileClip(url)
        s0, s1 = self._get_source_subclip_bounds(seg.segment_data, base.duration)
        # Use v1-compatible API for broader compatibility; v2 may accept subclipped in future
        base = base.subclip(s0, min(s1, base.duration))
        # timing
        base = base.with_start(seg.start_time).with_end(seg.end_time)
        # speed (pitch/time change; MoviePy speedx changes duration & pitch)
        sp = self._speed_factor(seg.segment_data)
        if abs(sp - 1.0) > 1e-6:
            try:
                base = base.with_speed(sp)
            except Exception:
                from moviepy import vfx as _vfx  # type: ignore
                base = base.fx(_vfx.speedx, sp)
        # volume
        cs = getattr(seg.segment_data, 'clip_settings', None)
        vol = float(getattr(cs, 'volume', 1.0) or 1.0) if cs is not None else 1.0
        if abs(vol - 1.0) > 1e-6:
            try:
                base = base.with_volume(vol)
            except Exception:
                base = base.volumex(vol)
        return base

    # ---------- transitions ----------
    def _apply_pull_transition(self, prev_clip, curr_clip, duration: float, mode: str = 'in', scale_s: float = 1.12):
        """
        Apply pull-in/pull-out between prev_clip and curr_clip over 'duration':
        - mode == 'in': A zooms 1.0‚ÜíS while fading out; B zooms S‚Üí1.0 while fading in
        - mode == 'out': A zooms 1.0‚Üí1/S while fading out; B zooms 1/S‚Üí1.0 while fading in

        Implemented by overlapping the two clips for 'duration' seconds:
        prev_clip: .crossfadeout(duration) + sized over its last 'duration'
        curr_clip: .crossfadein(duration)  + sized over its first 'duration'
        """
        if duration <= 0:
            return prev_clip, curr_clip

        # Overlap is already implied if start times butt-join; ensure overlap:
        # We'll leave starts as-is and rely on crossfade to blend.

        def prev_resize(t):
            if mode == 'in':
                # over last d seconds, t runs [T-d, T]; map to [0,1]
                # MoviePy passes t in clip time; we only want the last 'duration'
                return 1.0 + (scale_s - 1.0) * min(max((t - (prev_clip.duration - duration)) / max(duration, 1e-6), 0.0), 1.0)
            else:
                # pull-out: 1.0 ‚Üí 1/S
                return 1.0 + ((1.0 / max(scale_s, 1e-6)) - 1.0) * min(max((t - (prev_clip.duration - duration)) / max(duration, 1e-6), 0.0), 1.0)

        def curr_resize(t):
            if mode == 'in':
                # first d seconds: S ‚Üí 1.0
                return scale_s - (scale_s - 1.0) * min(max(t / max(duration, 1e-6), 0.0), 1.0)
            else:
                # pull-out: 1/S ‚Üí 1.0
                return (1.0 / max(scale_s, 1e-6)) + (1.0 - (1.0 / max(scale_s, 1e-6))) * min(max(t / max(duration, 1e-6), 0.0), 1.0)

        prev_z = prev_clip.fx(vfx.resize, lambda t: prev_resize(t)).crossfadeout(duration)
        curr_z = curr_clip.fx(vfx.resize, lambda t: curr_resize(t)).crossfadein(duration)
        return prev_z, curr_z

    def _extract_transition(self, prev_seg: CompositionSegment, curr_seg: CompositionSegment) -> Tuple[str, float]:
        # prefer normalized enum based on LUT
        trans_name_str = getattr(getattr(prev_seg, 'segment_data', None), '_cc_transition_enum_name', None) or \
                         getattr(getattr(curr_seg, 'segment_data', None), '_cc_transition_enum_name', None)
        trans_dur_sec = getattr(getattr(prev_seg, 'segment_data', None), '_cc_transition_duration_sec', None) or \
                        getattr(getattr(curr_seg, 'segment_data', None), '_cc_transition_duration_sec', None)
        if trans_name_str is None:
            prev_trans_obj = getattr(getattr(prev_seg, 'segment_data', None), 'transition', None)
            curr_trans_obj = getattr(getattr(curr_seg, 'segment_data', None), 'transition', None)
            def _name_dur(obj):
                if obj is None: return None, None
                name = None
                for attr in ('name', 'transition_name', 'type'):
                    try:
                        v = getattr(obj, attr, None)
                        if isinstance(v, str) and v.strip():
                            name = v.strip()
                            break
                    except Exception:
                        pass
                d = None
                for attr in ('duration', 'duration_us', 'duration_microseconds', 'duration_ms'):
                    try:
                        val = getattr(obj, attr, None)
                        if isinstance(val, (int, float)):
                            if attr.endswith(('us', 'microseconds')) or val > 10000: d = float(val) / 1_000_000.0
                            elif attr.endswith('ms'): d = float(val) / 1000.0
                            else: d = float(val)
                            break
                    except Exception:
                        pass
                return name, d
            n, d = _name_dur(prev_trans_obj)
            if not n:
                n, d = _name_dur(curr_trans_obj)
            trans_name_str = n
            trans_dur_sec = trans_dur_sec if trans_dur_sec is not None else d

        if trans_dur_sec is None:
            trans_dur_sec = 0.0
        name_norm = str(trans_name_str or "").strip().lower().replace(" ", "_")
        enum_name = TRANSITION_NAME_LUT.get(name_norm, name_norm)
        return enum_name, float(trans_dur_sec or 0.0)

    # ---------- composition ----------
    def compose(self) -> Tuple[CompositeVideoClip, Optional[CompositeAudioClip]]:
        bg = ColorClip(size=(self.width, self.height), color=(0, 0, 0), duration=self.duration_seconds)

        visual_clips: List[Any] = [bg.with_start(0).with_end(self.duration_seconds)]
        audio_clips: List[Any] = []

        # Build per-segment clips
        built_visual_by_seg: Dict[CompositionSegment, Any] = {}
        for seg in self.segments:
            try:
                if seg.track_type in ('video', 'image'):
                    vc = self._build_video_clip(seg)
                    if vc:
                        # transitions with previous on the same track if butt-join
                        prev = self.last_visual_by_track.get(seg.track_name)
                        if prev and abs(prev.end_time - seg.start_time) < 1e-4:
                            enum_name, d = self._extract_transition(prev, seg)
                            is_pull_in = (enum_name == 'Pull_in')
                            is_pull_out = (enum_name == 'Pull_Out')
                            if d > 0 and (is_pull_in or is_pull_out):
                                mode = 'in' if is_pull_in else 'out'
                                prev_clip = built_visual_by_seg.get(prev)
                                if prev_clip is not None:
                                    prev_z, curr_z = self._apply_pull_transition(prev_clip, vc, d, mode=mode, scale_s=1.12)
                                    # replace prev in list, add curr
                                    # Remove old prev from visual_clips
                                    if prev_clip in visual_clips:
                                        visual_clips.remove(prev_clip)
                                    visual_clips.extend([prev_z, curr_z])
                                    built_visual_by_seg[prev] = prev_z
                                    built_visual_by_seg[seg] = curr_z
                                else:
                                    visual_clips.append(vc)
                                    built_visual_by_seg[seg] = vc
                            else:
                                visual_clips.append(vc)
                                built_visual_by_seg[seg] = vc
                        else:
                            visual_clips.append(vc)
                            built_visual_by_seg[seg] = vc

                        self.last_visual_by_track[seg.track_name] = seg

                elif seg.track_type == 'text':
                    tc = self._build_text_clip(seg)
                    if tc:
                        visual_clips.append(tc)

                elif seg.track_type == 'sticker':
                    sc = self._build_sticker_clip(seg)
                    if sc:
                        visual_clips.append(sc)

                elif seg.track_type == 'audio':
                    ac = self._build_audio_clip(seg)
                    if ac:
                        audio_clips.append(ac)

            except Exception as e:
                logger.warning(f"Segment build failed (track={seg.track_name}, type={seg.track_type}): {e}")

        # Composite visuals in z-order (MoviePy resolves z from add order; we already sorted)
        video = CompositeVideoClip(visual_clips, size=(self.width, self.height)).with_duration(self.duration_seconds)

        # Audio mix
        audio = None
        if audio_clips:
            try:
                audio = CompositeAudioClip(audio_clips)
                video = video.set_audio(audio)
            except Exception as e:
                logger.warning(f"Audio mix failed, exporting silent video: {e}")

        return video, (audio if audio_clips else None)


# ---------------------------------------
# Export API (public)
# ---------------------------------------
def export_to_video_impl(
    output_path: str,
    yaml_config: Optional[str] = None,
    draft_id: Optional[str] = None,
    export_config: Optional[VideoExportConfig] = None
) -> Dict[str, Any]:

    if yaml_config and draft_id:
        raise ValueError("Cannot specify both yaml_config and draft_id")
    if not yaml_config and not draft_id:
        raise ValueError("Must specify either yaml_config or draft_id")

    if export_config is None:
        export_config = VideoExportConfig(output_path=output_path)

    try:
        logger.info(f"Starting MoviePy export ‚Üí {output_path}")

        # Build/resolve draft_id if YAML provided
        if yaml_config:
            from CapCutAPI import parse_yaml_config
            if os.path.isfile(yaml_config):
                result = parse_yaml_config(yaml_config)
            else:
                with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                    f.write(yaml_config)
                    temp_yaml_path = f.name
                try:
                    result = parse_yaml_config(temp_yaml_path)
                finally:
                    if os.path.exists(temp_yaml_path):
                        os.unlink(temp_yaml_path)
            draft_id = result.get('draft_id')
            if not draft_id:
                raise ValueError("Failed to parse YAML config or create draft")

        # Load script
        script = query_script_impl(draft_id, force_update=True)
        if script is None:
            raise ValueError(f"Draft {draft_id} not found")

        # Default width/height to script when not provided
        if export_config.width is None:
            export_config.width = script.width
        if export_config.height is None:
            export_config.height = script.height

        engine = MoviePyCompositionEngine(script, export_config)
        video, audio = engine.compose()

        # Write out
        # MoviePy write_videofile args
        write_kwargs = dict(
            codec=export_config.codec,
            preset=export_config.preset,
            bitrate=export_config.video_bitrate,
            audio_codec=(export_config.audio_codec if audio is not None else None),
            audio_bitrate=(export_config.audio_bitrate if audio is not None else None),
            audio_fps=export_config.audio_sample_rate if audio is not None else None,
            audio_nbytes=2,
            fps=export_config.fps,
            threads=os.cpu_count() or 4,
            temp_audiofile=os.path.join(tempfile.gettempdir(), "mpy_temp_audio.m4a") if audio is not None else None,
            remove_temp=True,
            write_logfile=False,
            verbose=False,
            logger=None,
        )

        # Ensure output directory exists
        out_dir = os.path.dirname(os.path.abspath(output_path))
        if out_dir and not os.path.exists(out_dir):
            os.makedirs(out_dir, exist_ok=True)

        video.write_videofile(output_path, **write_kwargs)

        # Stats
        file_size = os.path.getsize(output_path) if os.path.exists(output_path) else 0
        return {
            "success": True,
            "output_path": output_path,
            "duration": engine.duration_seconds,
            "width": engine.width,
            "height": engine.height,
            "fps": engine.fps,
            "file_size": file_size,
        }

    except Exception as e:
        logger.error(f"MoviePy export failed: {e}", exc_info=True)
        return {"success": False, "error": str(e)}


# ---------------------------------------
# CLI
# ---------------------------------------
def create_cli_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Export CapCut drafts to video using MoviePy",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python export_moviepy.py output.mp4 --yaml-config project.yml
  python export_moviepy.py output.mp4 --yaml-config 'draft:\\n  width: 1080\\nsteps:\\n  - add_text:\\n      text: "Hello"'
  python export_moviepy.py output.mp4 --draft-id dfd_cat_123 --width 1920 --height 1080 --fps 60
        """
    )
    parser.add_argument("output_path", help="Path for the output video")

    ig = parser.add_mutually_exclusive_group(required=True)
    ig.add_argument("--yaml-config", help="Path to YAML config or raw YAML string")
    ig.add_argument("--draft-id", help="Draft ID from cache")

    cg = parser.add_argument_group("export configuration")
    cg.add_argument("--width", type=int, help="Output width (default: draft width)")
    cg.add_argument("--height", type=int, help="Output height (default: draft height)")
    cg.add_argument("--fps", type=int, help="FPS (default: 30)")
    cg.add_argument("--video-bitrate", help="Video bitrate (e.g., 8000k)")
    cg.add_argument("--audio-bitrate", help="Audio bitrate (e.g., 128k)")
    cg.add_argument("--audio-codec", help="Audio codec (default: aac)")
    cg.add_argument("--audio-channels", type=int, help="Audio channels (default: 2)")
    cg.add_argument("--audio-sample-rate", type=int, help="Audio sample rate (default: 44100)")
    cg.add_argument("--codec", help="Video codec (default: libx264)")
    cg.add_argument("--preset", choices=[
        'ultrafast','superfast','veryfast','faster','fast','medium','slow','slower','veryslow'
    ], help="Encoding preset (default: medium)")
    cg.add_argument("--crf", help="CRF (MoviePy may ignore if bitrate set)")

    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose logging")
    parser.add_argument("--quiet", "-q", action="store_true", help="Quiet mode")

    return parser


def main():
    parser = create_cli_parser()
    args = parser.parse_args()

    if args.quiet:
        logging.basicConfig(level=logging.ERROR)
    elif args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)

    try:
        output_path = args.output_path

        cfg = VideoExportConfig(
            output_path=output_path,
            width=args.width,
            height=args.height,
            fps=args.fps or 30,
            video_bitrate=args.video_bitrate or "8000k",
            audio_bitrate=args.audio_bitrate or "128k",
            audio_codec=args.audio_codec or "aac",
            audio_channels=args.audio_channels or 2,
            audio_sample_rate=args.audio_sample_rate or 44100,
            codec=args.codec or "libx264",
            preset=args.preset or "medium",
            crf=args.crf or "23",
        )

        result = export_to_video_impl(
            output_path=output_path,
            yaml_config=args.yaml_config,
            draft_id=args.draft_id,
            export_config=cfg
        )

        if result["success"]:
            if not args.quiet:
                print("‚úÖ MoviePy export complete")
                print(f"üìÅ Output: {result['output_path']}")
                print(f"üé¨ Duration: {result['duration']:.2f}s")
                print(f"üìê Resolution: {result['width']}x{result['height']} @ {result['fps']} fps")
                if result.get("file_size", 0) > 0:
                    print(f"üíæ Size: {result['file_size']/1024/1024:.2f} MB")
            sys.exit(0)
        else:
            print(f"‚ùå Export failed: {result['error']}")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Interrupted")
        sys.exit(130)
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        if args.verbose:
            import traceback; traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()